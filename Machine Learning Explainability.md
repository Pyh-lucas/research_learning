[Learn Machine Learning Explainability | Kaggle](https://www.kaggle.com/learn/machine-learning-explainability)

	Extract human-understandable insights from any model.
---
## 1. Use cases ä»€ä¹ˆæ—¶å€™éœ€è¦ï¼Ÿ

	Â "black boxes"ï¼š you can't understand the logic behind those predictions

### 01 ==ç›®å‰å¯è§£é‡Šæ€§èƒ½å¤Ÿè·å¾—è¿™äº›insights==
- What features in the data did the model think are most important?ä»€ä¹ˆç‰¹å¾æ˜¯æœ€é‡è¦çš„
- For any single prediction from a model, how did each feature in the data affect that particular prediction?å¯¹äºæ¨¡å‹ä¸­çš„ä»»ä½•ä¸€ä¸ªé¢„æµ‹ï¼Œæ•°æ®ä¸­çš„æ¯ä¸ªç‰¹å¾å¦‚ä½•å½±å“è¯¥ç‰¹å®šé¢„æµ‹ï¼Ÿ
- How does each feature affect the model's predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions)?ä»å®è§‚ä¸Šçœ‹ï¼Œæ¯ä¸ªç‰¹å¾å¦‚ä½•å½±å“æ¨¡å‹çš„é¢„æµ‹ï¼ˆå½“è€ƒè™‘å¤§é‡å¯èƒ½çš„é¢„æµ‹æ—¶ï¼Œå®ƒçš„å…¸å‹æ•ˆæœæ˜¯ä»€ä¹ˆï¼‰ï¼Ÿ


### 02 ==è¿™äº›insightsçš„å¥½å¤„==

- Debugging
- Informing feature engineering
- Directing future data collection
- Informing human decision-making
- Building Trust

##### Debugging
ä¸å¯é çš„è„æ•°æ®å¯èƒ½åœ¨æ•°æ®é¢„å¤„ç†æ—¶å€™æ·»åŠ æ½œåœ¨çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒæ—¶å¯èƒ½å‡ºç°ç›®æ ‡æ³„éœ²çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¾ˆå¸¸è§ã€‚==Debugå¯ä»¥å‘ç°æ¨¡å‹æŒ–æ˜çš„patternå’Œç°å®ä¸–ç•Œæ˜¯å¦ä¸ä¸€è‡´ï¼Œè¿™æ˜¯è¿½è¸ªé”™è¯¯çš„ç¬¬ä¸€æ­¥==
	The world has a lot of unreliable, disorganized and generally dirty data. You add a potential source of errors as you write preprocessing code. Add in the potential forÂ [target leakage](https://www.kaggle.com/alexisbcook/data-leakage), and it is the norm rather than the exception to have errors at some point in a real data science project.

##### Informing Feature Engineering
	ç‰¹å¾å·¥ç¨‹æ˜¯æå‡æ¨¡å‹ç²¾åº¦æœ€æœ‰æ•ˆçš„æ–¹æ³•
[Feature engineering](https://www.kaggle.com/learn/feature-engineering)Â is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created.

	å°½ç®¡ä¼˜åŠ¿å¯ä»¥å‡­å€Ÿå¯¹æ½œåœ¨ä¸»é¢˜çš„ç›´è§‰æ¥å®Œæˆï¼Œç„¶è€Œå½“ç‰¹å¾å¢å¤š/ç¼ºä¹æ­£åœ¨ç ”ç©¶ä¸»é¢˜çš„èƒŒæ™¯çŸ¥è¯†æ—¶ï¼Œéœ€è¦è¿™äº›çŸ¥è¯†æŒ‡å¯¼
Sometimes you can go through this process using nothing but intuition about the underlying topic. But you'll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on.

	kaggleä¸­çš„ä¸€åœºæ¯”èµ›ä¸­ï¼Œç‰¹å¾åç§°æœªç»™å‡ºï¼Œå› æ­¤èƒŒæ™¯çŸ¥è¯†æ²¡æ³•ç”¨ã€‚ä¸¤ä¸ªåŠŸèƒ½ä¹‹é—´çš„å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯â€œf527 - f528â€ï¼Œåˆ›é€ äº†ä¸€ä¸ªéå¸¸å¼ºå¤§çš„æ–°åŠŸèƒ½ã€‚å°†è¿™ç§å·®å¼‚ä½œä¸ºä¸€é¡¹åŠŸèƒ½çš„æ¨¡å‹æ¯”æ²¡æœ‰å®ƒçš„æ¨¡å‹è¦å¥½å¾—å¤šã€‚ä½†æ˜¯ï¼Œå½“æ‚¨ä»æ•°ç™¾ä¸ªå˜é‡å¼€å§‹æ—¶ï¼Œæ‚¨ä¼šå¦‚ä½•è€ƒè™‘åˆ›å»ºæ­¤å˜é‡ï¼Ÿ
A Kaggle competition toÂ [predict loan defaults](https://www.kaggle.com/c/loan-default-prediction)Â gives an extreme example. This competition had 100s of raw features. For privacy reasons, the features had names likeÂ `f1`,Â `f2`,Â `f3`Â rather than common English names. This simulated a scenario where you have little intuition about the raw data.

	å› æ­¤å¯è§£é‡Šçš„æŠ€èƒ½èƒ½å¤Ÿå¸®åŠ©æ‰¾åˆ°é‡è¦ç‰¹å¾
The techniques you'll learn in this micro-course would make it transparent thatÂ `f527`Â andÂ `f528`Â are important features, and that their role is tightly entangled. This will direct you to consider transformations of these two variables, and likely find the "golden feature" ofÂ `f527 - f528`.




##### Directing Future Data Collection
	çº¿ä¸‹ç»„ç»‡å¯ä»¥é€šè¿‡ä½ ç›®å‰æŒ–æ˜å‡ºçš„æ¨¡å‹è§è§£ï¼Œäº†è§£é‚£äº›ç‰¹å¾æ›´é‡è¦ï¼Œä»è€Œå¸®åŠ©ä¼ä¸šæœªæ¥çš„æ•°æ®æ”¶é›†
	
#å†™ä½œå‚è€ƒ 
You have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful.


##### Informing Human Decision-Making

Some decisions are made automatically by models. Amazon doesn't have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. ==For these decisions, insights can be more valuable than predictions.==

##### Building Trust
å±•ç¤ºæ¨¡å‹å’Œç°å®ç†è§£ç›¸åŒ
Many people won't assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science.

---
## 2. Permutation Importanceï¼ˆä½ çš„æ¨¡å‹è®¤ä¸ºå“ªäº›ç‰¹å¾æ˜¯é‡è¦çš„ï¼Ÿï¼‰

	å…³äºâ€œå“ªäº›ç‰¹å¾å¯¹é¢„æµ‹å½±å“æœ€å¤§â€çš„é—®é¢˜ï¼Œä¸€èˆ¬ä½¿ç”¨feature importanceè¿›è¡Œè¡¡é‡ã€‚ç›®å‰æœ‰å¾ˆå¤šç§æ–¹æ³•ä»ç•¥å¾®ä¸åŒçš„versionå›ç­”äº†è¿™ä¸ªé—®é¢˜ï¼Œç„¶è€ŒåŒæ—¶å…·æœ‰ä¸€å®šç¼ºç‚¹ã€‚
	æ­¤å¤„ä½¿ç”¨Permutation Importanceæ¥è¡¡é‡ï¼Œè¯¥æ–¹æ³•çš„ä¼˜åŠ¿fast to calculate,widely used and understood, and consistent with properties we would want a feature importance measure to have.

### 01 åŸç†

	æƒ³è¦é¢„æµ‹ä¸€ä¸ªäºº20å²æ—¶çš„èº«é«˜ã€‚æˆ‘ä»¬çš„æ•°æ®åŒ…æ‹¬æœ‰ç”¨çš„ç‰¹å¾ï¼ˆ10 å²æ—¶çš„èº«é«˜ï¼‰ã€é¢„æµ‹èƒ½åŠ›ä¸å¼ºçš„ç‰¹å¾ï¼ˆæ‹¥æœ‰çš„è¢œå­ï¼‰ä»¥åŠæˆ‘ä»¬åœ¨æœ¬è§£é‡Šä¸­ä¸ä¼šé‡ç‚¹å…³æ³¨çš„å…¶ä»–ä¸€äº›ç‰¹å¾ã€‚
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071424406.png?imageSlim)
==ç½®æ¢é‡è¦æ€§æ˜¯åœ¨æ‹Ÿåˆæ¨¡å‹åè®¡ç®—çš„ã€‚==å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šæ”¹å˜æ¨¡å‹æˆ–æ”¹å˜æˆ‘ä»¬å¯¹ç»™å®šçš„èº«é«˜ã€è¢œå­æ•°ç­‰å€¼çš„é¢„æµ‹ã€‚

ç›¸åï¼Œæˆ‘ä»¬å°†æå‡ºä»¥ä¸‹é—®é¢˜ï¼š==å¦‚æœæˆ‘éšæœºéšæœºæ´—ç‰ŒéªŒè¯æ•°æ®çš„ä¸€åˆ—ï¼Œå°†ç›®æ ‡å’Œæ‰€æœ‰å…¶ä»–åˆ—ä¿ç•™åœ¨åŸä½ï¼Œè¿™å°†å¦‚ä½•å½±å“ç°åœ¨æ´—ç‰Œæ•°æ®ä¸­é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Ÿ==
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071425167.png?imageSlim)
éšæœºé‡æ–°æ’åºå•ä¸ªåˆ—ä¼šå¯¼è‡´é¢„æµ‹çš„å‡†ç¡®æ€§é™ä½ï¼Œå› ä¸ºç”Ÿæˆçš„æ•°æ®ä¸å†ä¸ç°å®ä¸–ç•Œä¸­è§‚å¯Ÿåˆ°çš„ä»»ä½•æ•°æ®ç›¸å¯¹åº”ã€‚==å¦‚æœæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œé¢„æµ‹æ‰€ä¸¥é‡ä¾èµ–çš„åˆ—è¿›è¡Œæ´—ç‰Œï¼Œåˆ™æ¨¡å‹å‡†ç¡®æ€§å°¤å…¶å—åˆ°å½±å“==ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåœ¨ 10 å²æ—¶æ”¹å˜èº«é«˜ä¼šå¯¼è‡´å¯æ€•çš„é¢„æµ‹ã€‚å¦‚æœæˆ‘ä»¬æ”¹ç”¨è‡ªæœ‰çš„è¢œå­ï¼Œé‚£ä¹ˆç”±æ­¤äº§ç”Ÿçš„é¢„æµ‹å°±ä¸ä¼šå—åˆ°å¤ªå¤§çš„å½±å“ã€‚

**å› æ­¤ï¼ŒPermutation Importanceçš„æµç¨‹å¦‚ä¸‹ï¼š**
- **è·å–ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ã€‚**
- **å¯¹å•åˆ—ä¸­çš„å€¼è¿›è¡Œéšæœºæ’åºï¼Œä½¿ç”¨ç”Ÿæˆçš„æ•°æ®é›†è¿›è¡Œé¢„æµ‹ã€‚ä½¿ç”¨è¿™äº›é¢„æµ‹å’ŒçœŸå®ç›®æ ‡å€¼æ¥è®¡ç®—æŸå¤±å‡½æ•°å› æ´—ç‰Œè€Œé­å—çš„æŸå¤±ã€‚==è¿™ç§æ€§èƒ½ä¸‹é™è¡¡é‡äº†æ‚¨åˆšåˆšæ´—ç‰Œçš„å˜é‡çš„é‡è¦æ€§ã€‚==**
- **å°†æ•°æ®æ¢å¤åˆ°åŸå§‹é¡ºåºï¼ˆæ’¤æ¶ˆæ­¥éª¤ 2 ä¸­çš„éšæœºæ’åºï¼‰ã€‚ç°åœ¨å¯¹æ•°æ®é›†ä¸­çš„ä¸‹ä¸€åˆ—é‡å¤æ­¥éª¤ 2ï¼Œç›´åˆ°è®¡ç®—å‡ºæ¯åˆ—çš„é‡è¦æ€§ã€‚**

	è¿™ç§æ€æƒ³å…¶å®å’Œå“ä¼¦å¸ˆå…„è¯´çš„æ£€æµ‹æ•°æ®good/bad caseçš„æ–¹æ³•æ€æƒ³æ˜¯ä¸€æ ·çš„


### 02 ğŸ
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071430177.png?imageSlim)

==eli5 å·²ç»ä¸èƒ½ç”¨äº†==
```python
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())
```


[4.2. Permutation feature importance â€” scikit-learn 1.4.1 documentation](https://scikit-learn.org/stable/modules/permutation_importance.html)
è§£è¯»è´Ÿå€¼[Learn Machine Learning Explainability | Kaggle](https://www.kaggle.com/learn/machine-learning-explainability/discussion/356240)
[Stop Permuting Features. Permutation importance may give youâ€¦ | by Denis Vorotyntsev | Towards Data Science](https://towardsdatascience.com/stop-permuting-features-c1412e31b63f)


![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071430741.png?imageSlim)

### 03 ç»“æœè§£è¯»

- The values towards the top are the most important features, and those towards the bottom matter least.==è¶Šé¡¶éƒ¨çš„è¶Šé‡è¦==

- The first number in each row shows how much model performance decreased with a random shuffling (in this case, using "accuracy" as the performance metric).ç¬¬ä¸€ä¸ªæ•°å­—è¡¨ç¤ºæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡åœ¨éšæœºæ‰“ä¹±åå¦‚ä½•==â€œä¸‹é™â€==

- Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. ==We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after theÂ **Â±**Â measures how performance varied from one-reshuffling to the next.==+-åé¢çš„æ•°å­—è¡¡é‡çš„æ˜¯è¿ç»­ä¸¤æ¬¡éšæœºæ‰“ä¹±æ—¶ï¼ŒæŒ‡æ ‡çš„æ³¢åŠ¨æ€§

- You'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.å‡ºç°è´Ÿæ•°çš„å“ªäº›ç‰¹å¾æ„å‘³ç€è¿™äº›ç‰¹å¾å¹¶ä¸é‡è¦ï¼Œé€šå¸¸å‡ºç°åœ¨å°æ•°æ®é›†ä¸­

	In our example, the most important feature wasÂ **Goals scored**. That seems sensible. Soccer fans may have some intuition about whether the orderings of other variables are surprising or not.

---
## 3. Partial Plotsï¼ˆç‰¹å¾æ˜¯å¦‚ä½•å½±å“é¢„æµ‹çš„ï¼Ÿï¼‰

### 01 Partial Dependence Plots éƒ¨åˆ†ä¾èµ–å…³ç³»å›¾

ç‰¹å¾é‡è¦æ€§æ˜¾ç¤ºå“ªäº›å˜é‡å¯¹é¢„æµ‹å½±å“æœ€å¤§ï¼Œè€Œ==éƒ¨åˆ†ä¾èµ–å…³ç³»å›¾æ˜¾ç¤ºç‰¹å¾å¦‚ä½•å½±å“é¢„æµ‹ï¼ˆå•ä½å˜åŠ¨å¦‚ä½•å½±å“æ•´ä½“æ•°å€¼ï¼Œä¸æ˜¯shuffleï¼‰==

This is useful to answer questions like:

- Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas?æ§åˆ¶æ‰€æœ‰å…¶ä»–æˆ¿å±‹ç‰¹å¾ï¼Œç»åº¦å’Œçº¬åº¦å¯¹æˆ¿ä»·æœ‰ä»€ä¹ˆå½±å“ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œä¸åŒåœ°åŒºç±»ä¼¼å¤§å°çš„æˆ¿å±‹å°†å¦‚ä½•å®šä»·ï¼Ÿ
    
- Are predicted health differences between two groups due to differences in their diets, or due to some other factor?ä¸¤ç»„ä¹‹é—´çš„é¢„æµ‹å¥åº·å·®å¼‚æ˜¯ç”±äºé¥®é£Ÿå·®å¼‚è¿˜æ˜¯ç”±äºå…¶ä»–å› ç´ ï¼Ÿ

If you are familiar with linear or logistic regression models, partial dependence plots can be interpreted similarly to the coefficients in those models. Though, partial dependence plots on sophisticated models can capture more complex patterns than coefficients from simple models. If you aren't familiar with linear or logistic regressions, don't worry about this comparison.å¦‚æœæ‚¨ç†Ÿæ‚‰çº¿æ€§å›å½’æˆ–é€»è¾‘å›å½’æ¨¡å‹ï¼Œåˆ™å¯ä»¥å°†éƒ¨åˆ†ä¾èµ–å›¾è§£é‡Šä¸ºä¸è¿™äº›æ¨¡å‹ä¸­çš„ç³»æ•°ç±»ä¼¼ã€‚ä½†æ˜¯ï¼Œä¸ç®€å•æ¨¡å‹ä¸­çš„ç³»æ•°ç›¸æ¯”ï¼Œå¤æ‚æ¨¡å‹ä¸Šçš„éƒ¨åˆ†ä¾èµ–æ€§å›¾å¯ä»¥æ•è·æ›´å¤æ‚çš„æ¨¡å¼ã€‚å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰çº¿æ€§å›å½’æˆ–é€»è¾‘å›å½’ï¼Œè¯·ä¸è¦æ‹…å¿ƒè¿™ç§æ¯”è¾ƒã€‚

We will show a couple examples, explain the interpretation of these plots, and then review the code to create these plots.


### 02 åŸç†

ä¸æ’åˆ—é‡è¦æ€§ä¸€æ ·ï¼Œéƒ¨åˆ†ä¾èµ–å›¾æ˜¯åœ¨æ‹Ÿåˆæ¨¡å‹åè®¡ç®—çš„ã€‚Like permutation importance,Â **partial dependence plots are calculated after a model has been fit.**Â The model is fit on real data that has not been artificially manipulated in any way.

åœ¨æˆ‘ä»¬çš„è¶³çƒç¤ºä¾‹ä¸­ï¼Œçƒé˜Ÿå¯èƒ½åœ¨è®¸å¤šæ–¹é¢æœ‰æ‰€ä¸åŒã€‚ä»–ä»¬ä¼ çƒäº†å¤šå°‘æ¬¡ï¼Œå°„é—¨æ¬¡æ•°å¤šäº†ï¼Œè¿›çƒäº†å¤šå°‘çƒï¼Œç­‰ç­‰ã€‚ä¹ä¸€çœ‹ï¼Œä¼¼ä¹å¾ˆéš¾è§£å¼€è¿™äº›ç‰¹å¾çš„å½±å“ã€‚

	ä¸ºäº†äº†è§£éƒ¨åˆ†å›¾å¦‚ä½•åˆ†ç¦»å‡ºæ¯ä¸ªç‰¹å¾çš„å½±å“ï¼Œæˆ‘ä»¬é¦–å…ˆè€ƒè™‘ä¸€è¡Œæ•°æ®ã€‚ä¾‹å¦‚ï¼Œè¯¥è¡Œæ•°æ®å¯èƒ½è¡¨ç¤ºä¸€æ”¯çƒé˜Ÿæœ‰ 50% çš„æ—¶é—´æ§çƒã€100 æ¬¡ä¼ çƒã€10 æ¬¡å°„é—¨å’Œ 1 ä¸ªè¿›çƒã€‚

æˆ‘ä»¬å°†ä½¿ç”¨å·²ç»è®­ç»ƒå¥½äº†çš„æ¨¡å‹æ¥é¢„æµ‹æˆ‘ä»¬çš„ç»“æœï¼ˆä»–ä»¬çš„çƒå‘˜èµ¢å¾—â€œæ¯”èµ›æœ€ä½³çƒå‘˜â€çš„æ¦‚ç‡ï¼‰ã€‚==ä½†æ˜¯æˆ‘ä»¬åå¤æ”¹å˜ä¸€ä¸ªå˜é‡çš„å€¼æ¥åšå‡ºä¸€ç³»åˆ—é¢„æµ‹==ã€‚å¦‚æœçƒé˜Ÿåªæœ‰40%çš„æ—¶é—´æ§çƒï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹ç»“æœã€‚ç„¶åæˆ‘ä»¬é¢„æµ‹ä»–ä»¬æœ‰ 50% çš„æ—¶é—´æœ‰çƒã€‚ç„¶åå†æ¬¡é¢„æµ‹ 60%ã€‚ç­‰ç­‰ã€‚==ç„¶åï¼ŒWe trace out predicted outcomes (on the vertical axisåœ¨æ¨ªè½´ä¸Š) as we move from small values of ball possession to large values (on the horizontal axisåœ¨çºµè½´ä¸Š).==

	åœ¨æ­¤æè¿°ä¸­ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨äº†å•è¡Œæ•°æ®ã€‚è¦ç´ ä¹‹é—´çš„äº¤äº’å¯èƒ½ä¼šå¯¼è‡´å•è¡Œçš„ç»˜å›¾ä¸å…¸å‹ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬ç”¨åŸå§‹æ•°æ®é›†ä¸­çš„å¤šè¡Œé‡å¤è¯¥å®éªŒï¼Œå¹¶åœ¨çºµè½´ä¸Šç»˜åˆ¶å¹³å‡é¢„æµ‹ç»“æœã€‚

### 03 ğŸ

![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071504859.png?imageSlim)
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071505030.png?imageSlim)
	Here is the code to create the Partial Dependence Plot using the scikit-learn library.


```python
from matplotlib import pyplot as plt
from sklearn.inspection import PartialDependenceDisplay

# Create and plot the data
disp1 = PartialDependenceDisplay.from_estimator(tree_model, val_X, ['Goal Scored'])
plt.show()
```

y è½´è¢«è§£é‡Šä¸ºé¢„æµ‹å€¼ç›¸å¯¹äºåŸºçº¿æˆ–æœ€å·¦ä¾§å€¼çš„é¢„æµ‹å€¼çš„å˜åŒ–
The y axis is interpreted asÂ **change in the prediction**Â from what it would be predicted at the baseline or leftmost value.

ä»è¿™å¼ ç‰¹æ®Šçš„å›¾è¡¨ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°è¿›çƒå¤§å¤§å¢åŠ äº†ä½ èµ¢å¾—â€œå…¨åœºæœ€ä½³çƒå‘˜â€çš„æœºä¼šã€‚ä½†é™¤æ­¤ä¹‹å¤–çš„é¢å¤–ç›®æ ‡ä¼¼ä¹å¯¹é¢„æµ‹å½±å“ä¸å¤§
From this particular graph, we see that scoring a goal substantially increases your chances of winning "Man of The Match." But extra goals beyond that appear to have little impact on predictions.

==é’ˆå¯¹Goal Scoreçš„å˜åŠ¨ï¼Œyè½´æ˜¯â€œèµ¢å¾—å…¨åœºæœ€ä½³çƒå‘˜â€å˜åŠ¨çš„å¹…åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ­¤å¤„éšç€Goal Scoreä»å·¦åˆ°å³çš„å˜åŠ¨ï¼Œâ€œèµ¢å¾—å…¨åœºæœ€ä½³çƒå‘˜â€çš„å¹…åº¦æå‡==
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071634229.png?imageSlim)
å¦ä¸€ä¸ªå˜é‡ï¼š
	è¿™å¼ å›¾ä¼¼ä¹å¤ªç®€å•äº†ï¼Œæ— æ³•ä»£è¡¨ç°å®ã€‚ä½†é‚£æ˜¯å› ä¸ºæ¨¡å‹å¤ªç®€å•äº†ã€‚æ‚¨åº”è¯¥èƒ½å¤Ÿä»ä¸Šé¢çš„å†³ç­–æ ‘ä¸­çœ‹åˆ°ï¼ˆ<101.5æ—¶ï¼Œä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼‰ï¼Œè¿™å®Œå…¨ä»£è¡¨äº†æ¨¡å‹çš„ç»“æ„ã€‚
	![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071642623.png?imageSlim)
	æ‚¨å¯ä»¥è½»æ¾åœ°æ¯”è¾ƒä¸åŒæ¨¡å‹çš„ç»“æ„æˆ–å«ä¹‰ã€‚è¿™æ˜¯ä¸éšæœºæ£®æ—æ¨¡å‹ç›¸åŒçš„å›¾ã€‚
	![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071647800.png?imageSlim)
	This model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.è¯¥æ¨¡å‹è®¤ä¸ºï¼Œå¦‚æœæ‚¨çš„çƒå‘˜åœ¨æ¸¸æˆè¿‡ç¨‹ä¸­æ€»å…±è·‘äº† 100 å…¬é‡Œï¼Œæ‚¨æ›´æœ‰å¯èƒ½èµ¢å¾—æ¯”èµ›æœ€ä½³çƒå‘˜ã€‚å°½ç®¡è¿è¡Œå¾—è¶Šå¤šä¼šå¯¼è‡´é¢„æµ‹å€¼é™ä½ã€‚
	In general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.ä¸€èˆ¬æ¥è¯´ï¼Œè¿™æ¡æ›²çº¿çš„å¹³æ»‘å½¢çŠ¶ä¼¼ä¹æ¯”å†³ç­–æ ‘æ¨¡å‹ä¸­çš„é˜¶è·ƒå‡½æ•°æ›´åˆç†ã€‚å°½ç®¡è¿™ä¸ªæ•°æ®é›†è¶³å¤Ÿå°ï¼Œä½†æˆ‘ä»¬åœ¨è§£é‡Šä»»ä½•æ¨¡å‹æ—¶éƒ½ä¼šå°å¿ƒã€‚

### 04 2Déƒ¨åˆ†ä¾èµ–å›¾ï¼ˆå¯»æ‰¾å±æ€§é—´çš„äº¤äº’æ•ˆåº”ï¼‰

[[10-ç‰¹å¾å·¥ç¨‹]] #äº¤äº’æ•ˆåº”

==å¦‚æœæ‚¨å¯¹ç‰¹å¾ä¹‹é—´çš„interactionæ„Ÿåˆ°å¥½å¥‡ï¼Œ2D éƒ¨åˆ†ä¾èµ–å…³ç³»å›¾ä¹Ÿå¾ˆæœ‰ç”¨==ã€‚ä¸€ä¸ªä¾‹å­å¯ä»¥æ¾„æ¸…è¿™ä¸€ç‚¹ã€‚If you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.

æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨å†³ç­–æ ‘æ¨¡å‹æ¥ç»˜åˆ¶æ­¤å›¾ã€‚å®ƒå°†åˆ›å»ºä¸€ä¸ªéå¸¸ç®€å•çš„å›¾ï¼Œä½†æ‚¨åº”è¯¥èƒ½å¤Ÿå°†æ‚¨åœ¨å›¾ä¸­çœ‹åˆ°çš„å†…å®¹ä¸æ ‘æœ¬èº«ç›¸åŒ¹é…ã€‚We will again use the Decision Tree model for this graph. It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.

```python
fig, ax = plt.subplots(figsize=(8, 6))
f_names = [('Goal Scored', 'Distance Covered (Kms)')]
# Similar to previous PDP plot except we use tuple of features instead of single feature
disp4 = PartialDependenceDisplay.from_estimator(tree_model, val_X, f_names, ax=ax)
plt.show()
```
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071651844.png?imageSlim)
This graph shows predictions for any combination of Goals Scored and Distance covered.æ­¤å›¾æ˜¾ç¤ºäº†å¯¹â€œè¿›çƒæ•°â€å’Œâ€œè¦†ç›–è·ç¦»â€çš„ä»»æ„ç»„åˆçš„é¢„æµ‹ã€‚çº¿ä¸Šçš„æŒ‡æ ‡æ˜¾ç¤ºçš„æ—¶é¢„æµ‹å˜é‡çš„æ€§èƒ½æŒ‡æ•°

For example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals?ä¾‹å¦‚ï¼Œå½“ä¸€æ”¯çƒé˜Ÿè‡³å°‘æ‰“è¿› 1 ä¸ªçƒå¹¶ä¸”ä»–ä»¬çš„æ€»è·ç¦»æ¥è¿‘ 100 å…¬é‡Œæ—¶ï¼ˆåœ¨äºŒç»´åæ ‡ä¸­ç¡®å®šä¸€ä¸ªç‚¹ï¼‰ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æœ€é«˜çš„é¢„æµ‹ã€‚å¦‚æœä»–ä»¬çš„è¿›çƒæ•°ä¸º0ï¼Œé‚£ä¹ˆè¦†ç›–çš„è·ç¦»å¹¶ä¸é‡è¦ã€‚ä½ èƒ½é€šè¿‡è¿½è¸ª 0 ä¸ªè¿›çƒçš„å†³ç­–æ ‘æ¥çœ‹åˆ°è¿™ä¸€ç‚¹å—ï¼Ÿ

But distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?ä½†æ˜¯ï¼Œå¦‚æœä»–ä»¬è¿›çƒï¼Œè·ç¦»ä¼šå½±å“é¢„æµ‹ã€‚ç¡®ä¿æ‚¨å¯ä»¥ä» 2D éƒ¨åˆ†ä¾èµ–å…³ç³»å›¾ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚ä½ èƒ½åœ¨å†³ç­–æ ‘ä¸­çœ‹åˆ°è¿™ç§æ¨¡å¼å—ï¼Ÿ

---
## 4. SHAP Values

	Understand individual predictions

### 01 åŠŸèƒ½

You've seen (and used) techniques to extract general insights from a machine learning model. But what if you want to break down how the model works for an individual prediction?ä½ å·²äº†è§£ï¼ˆå¹¶ä½¿ç”¨ï¼‰ä»æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­æå–generalè§è§£çš„æŠ€æœ¯ã€‚ä½†æ˜¯ï¼Œ==å¦‚æœæ‚¨æƒ³åˆ†è§£æ¨¡å‹å¦‚ä½•ç”¨äºå•ä¸ªé¢„æµ‹ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ==

SHAP å€¼ï¼ˆSHapley Additive exPlanations çš„é¦–å­—æ¯ç¼©å†™ï¼‰å¯¹é¢„æµ‹è¿›è¡Œç»†åˆ†ï¼Œä»¥æ˜¾ç¤ºæ¯ä¸ªç‰¹å¾çš„å½±å“ã€‚ä½¿ç”¨åœºæ™¯å¦‚ä¸‹ï¼š

- ä¸€ä¸ªæ¨¡å‹è¯´ï¼Œé“¶è¡Œä¸åº”è¯¥å€Ÿé’±ç»™åˆ«äººï¼Œæ³•å¾‹è¦æ±‚é“¶è¡Œè§£é‡Šæ¯æ¬¡è´·æ¬¾è¢«æ‹’ç»çš„ä¾æ®
- åŒ»ç–—ä¿å¥æä¾›è€…å¸Œæœ›ç¡®å®šå“ªäº›å› ç´ å¯¼è‡´æ¯ä½æ‚£è€…æ‚£æŸç§ç–¾ç—…çš„é£é™©ï¼Œä»¥ä¾¿ä»–ä»¬å¯ä»¥é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¥åº·å¹²é¢„æªæ–½ç›´æ¥è§£å†³è¿™äº›é£é™©å› ç´ 

åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ SHAP å€¼æ¥è§£é‡Šå„ä¸ªé¢„æµ‹ã€‚åœ¨ä¸‹ä¸€è¯¾ä¸­ï¼Œä½ å°†äº†è§£å¦‚ä½•å°†è¿™äº›å†…å®¹èšåˆä¸ºå¼ºå¤§çš„æ¨¡å‹çº§è§è§£ã€‚

### 02 åŸç†

SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.  SHAP å€¼èƒ½å¤Ÿè§£é‡Šï¼Œå½“ç»™å®šç‰¹å¾å…·ä½“å€¼æ—¶ï¼Œä¸è¯¥ç‰¹å¾é‡‡ç”¨æŸä¸ªåŸºçº¿å€¼æ—¶ï¼Œæˆ‘ä»¬æ‰€åšçš„é¢„æµ‹çš„å¯¹æ¯”å½±å“

An example is helpful, and we'll continue the soccer/football example from the permutation importance and partial dependence plots lessons.
In these tutorials, we predicted whether a team would have a player win the Man of the Match award.ã€

We could ask:æˆ‘ä»¬å¯èƒ½ä¼šé—®
å½“ä¸€æ”¯çƒé˜Ÿå·²ç»è¿›äº†3çƒï¼Œé¢„æµ‹çš„ç»“æœå¦‚ä½•ï¼Ÿ
How much was a prediction driven by the fact that the team scored 3 goals?

But it's easier to give a concrete, numeric answer if we restate this as:
åœ¨å·²ç»è¿›äº†3çƒçš„æƒ…å†µä¸‹ï¼Œç°æœ‰é¢„æµ‹æ˜¯å¤šå°‘ï¼Œè€Œéå…¶ä»–è¿›çƒçš„baselineé˜ˆå€¼
How much was a prediction driven by the fact that the team scored 3 goals, instead of some baseline number of goals.

Of course, each team has many features. So if we answer this question for number of goals, we could repeat the process for all other features.å½“ç„¶ï¼Œæ¯ä¸ªå›¢é˜Ÿéƒ½æœ‰å¾ˆå¤šç‰¹ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªç‰¹å¾é‡å¤è®¡ç®—SHAP

SHAP values do this in a way that guarantees a nice property. Specifically, you decompose a prediction with the following equation:

	sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values

That is, the SHAP values of all features sum up to explain why my prediction was different from the baseline. This allows us to decompose a prediction in a graph like this:ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰€æœ‰ç‰¹å¾çš„ SHAP å€¼ç›¸åŠ ä»¥è§£é‡Šä¸ºä»€ä¹ˆæˆ‘çš„é¢„æµ‹ä¸åŸºçº¿ä¸åŒã€‚è¿™å…è®¸æˆ‘ä»¬åœ¨å›¾ä¸­åˆ†è§£é¢„æµ‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071721487.png?imageSlim)

How do you interpret this?

We predicted 0.7, whereas the base_value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes fromÂ `Goal Scored`Â being 2. Though the ball possession value has a meaningful effect decreasing the prediction.æˆ‘ä»¬çš„é¢„æµ‹å€¼æ˜¯ 0.7ï¼Œè€ŒåŸºå‡†å€¼æ˜¯ 0.4979ã€‚å¯¼è‡´é¢„æµ‹å€¼å¢åŠ çš„ç‰¹å¾å€¼ä¸ºç²‰çº¢è‰²ï¼Œå…¶è§†è§‰å¤§å°è¡¨ç¤ºç‰¹å¾å½±å“çš„ç¨‹åº¦ã€‚é™ä½é¢„æµ‹å€¼çš„ç‰¹å¾å€¼ä¸ºè“è‰²ã€‚å½±å“æœ€å¤§çš„æ˜¯è¿›çƒæ•°ä¸º 2 çš„ç‰¹å¾å€¼ã€‚å°½ç®¡æ§çƒç‡å€¼å¯¹é™ä½é¢„æµ‹å€¼ä¹Ÿæœ‰ä¸€å®šå½±å“ã€‚(æˆ‘è®¤ä¸ºè¿™é‡Œçš„predictionå¯èƒ½æŒ‡çš„æ˜¯å…·ä½“çš„å€¼ï¼Œä¹Ÿå°±æ˜¯è¯´èµ·åˆ°å¯¹å€¼æœ¬èº«èµ·åˆ°åä½œç”¨ï¼Œè€Œä¸æ˜¯è¯´è¿™ä¸ªç‰¹å¾å†—ä½™äº†)

If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.å¦‚æœç”¨ç²‰è‰²é•¿æ¡çš„é•¿åº¦å‡å»è“è‰²é•¿æ¡çš„é•¿åº¦ï¼Œå°±ç­‰äºä»åŸºç¡€å€¼åˆ°è¾“å‡ºå€¼çš„è·ç¦»ã€‚

There is some complexity to the technique, to ensure that the baseline plus the sum of individual effects adds up to the prediction (which isn't as straightforward as it sounds). We won't go into that detail here, since it isn't critical for using the technique.Â [This blog post](https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d)Â has a longer theoretical explanation.è¿™é¡¹æŠ€æœ¯æœ‰ä¸€å®šçš„å¤æ‚æ€§ï¼Œè¦ç¡®ä¿åŸºçº¿åŠ ä¸Šå•ä¸ªå½±å“çš„æ€»å’Œç­‰äºé¢„æµ‹å€¼ï¼ˆè¿™å¹¶ä¸åƒå¬èµ·æ¥é‚£ä¹ˆç®€å•ï¼‰ã€‚ç”±äºè¿™å¯¹ä½¿ç”¨è¯¥æŠ€æœ¯å¹¶ä¸é‡è¦ï¼Œæˆ‘ä»¬åœ¨æ­¤å°±ä¸èµ˜è¿°äº†ã€‚è¿™ç¯‡åšæ–‡æœ‰è¾ƒé•¿çš„ç†è®ºè§£é‡Šã€‚

### 03 ğŸ

[shap/shap: A game theoretic approach to explain the output of any machine learning model. (github.com)](https://github.com/shap/shap)
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071726823.png?imageSlim)

![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071726418.png?imageSlim)

```python
import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# Calculate Shap values
shap_values = explainer.shap_values(data_for_prediction)
```

TheÂ `shap_values`Â object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don't win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we'll pull out SHAP values for positive outcomes (pulling outÂ `shap_values[1]`).
ä¸Šè¿° shap_values å¯¹è±¡æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ•°ç»„çš„åˆ—è¡¨ã€‚ç¬¬ä¸€ä¸ªæ•°ç»„æ˜¯è´Ÿé¢ç»“æœï¼ˆæœªè·å¥–ï¼‰çš„ SHAP å€¼ï¼Œç¬¬äºŒä¸ªæ•°ç»„æ˜¯æ­£é¢ç»“æœï¼ˆè·å¥–ï¼‰çš„ SHAP å€¼åˆ—è¡¨ã€‚æˆ‘ä»¬é€šå¸¸ä¼šä»æ­£é¢ç»“æœçš„é¢„æµ‹è§’åº¦æ¥è€ƒè™‘é¢„æµ‹ï¼Œå› æ­¤æˆ‘ä»¬ä¼šå–å‡ºæ­£é¢ç»“æœçš„ SHAP å€¼ï¼ˆå–å‡º shap_values[1]ï¼‰ã€‚

It's cumbersome to review raw arrays, but the shap package has a nice way to visualize the results.
æŸ¥çœ‹åŸå§‹æ•°ç»„å¾ˆéº»çƒ¦ï¼Œä½† shap è½¯ä»¶åŒ…æœ‰ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•æ¥å¯è§†åŒ–ç»“æœã€‚

```python
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)
```

![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071729125.png?imageSlim)

If you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in shap.TreeExplainer(my_model). But the SHAP package has explainers for every type of model.==å¦‚æœä»”ç»†æŸ¥çœ‹æˆ‘ä»¬åˆ›å»º SHAP å€¼çš„ä»£ç ï¼Œå°±ä¼šå‘ç°æˆ‘ä»¬åœ¨ shap.TreeExplainer(my_model) ä¸­å¼•ç”¨äº†æ ‘ã€‚ä½†æ˜¯ï¼ŒSHAP è½¯ä»¶åŒ…ä¸ºæ¯ç§ç±»å‹çš„æ¨¡å‹éƒ½æä¾›äº†è§£é‡Šå™¨ã€‚==

- `shap.DeepExplainer`Â works with Deep Learning models.é€‚ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
- `shap.KernelExplainer`Â works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.é€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼Œä¸è¿‡å®ƒæ¯”å…¶ä»–è§£é‡Šå™¨æ…¢ï¼Œè€Œä¸”æä¾›çš„æ˜¯è¿‘ä¼¼å€¼è€Œä¸æ˜¯ç²¾ç¡®çš„ Shap å€¼ã€‚

Here is an example using KernelExplainer to get similar results. The results aren't identical because KernelExplainer gives an approximate result. But the results tell the same story.ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ KernelExplainer è·å¾—ç±»ä¼¼ç»“æœçš„ç¤ºä¾‹ã€‚ç”±äº KernelExplainer å¾—å‡ºçš„æ˜¯è¿‘ä¼¼å€¼ï¼Œå› æ­¤ç»“æœå¹¶ä¸å®Œå…¨ç›¸åŒã€‚ä½†ç»“æœè¯´æ˜çš„é—®é¢˜æ˜¯ä¸€æ ·çš„ã€‚

```python
# use Kernel SHAP to explain test set predictions
k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)
k_shap_values = k_explainer.shap_values(data_for_prediction)
shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)
```

![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071737655.png?imageSlim)
 ![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071737625.png?imageSlim)

---
## 5. Advanced Uses of SHAP Values

	Aggregate SHAP values for even more detailed model insights

	We started by learning about permutation importance and partial dependence plots for an overview of what the model has learned.æˆ‘ä»¬é¦–å…ˆå­¦ä¹ äº†æ’åˆ—é‡è¦æ€§å’Œéƒ¨åˆ†ä¾èµ–å›¾ï¼Œä»¥äº†è§£æ¨¡å‹æ‰€å­¦åˆ°çš„çŸ¥è¯†ã€‚
	We then learned about SHAP values to break down the components of individual predictions.ç„¶åï¼Œæˆ‘ä»¬å­¦ä¹ äº† SHAP å€¼ï¼Œä»¥åˆ†è§£å•ä¸ªé¢„æµ‹çš„ç»„æˆéƒ¨åˆ†ã€‚
	Now we'll expand on SHAP values, seeing how aggregating many SHAP values can give more detailed alternatives to permutation importance and partial dependence plots.ç°åœ¨ï¼Œæˆ‘ä»¬å°†æ‰©å±• SHAP å€¼ï¼Œäº†è§£å°†å¤šä¸ª SHAP å€¼èšåˆåœ¨ä¸€èµ·å¦‚ä½•ä¸ºæ’åˆ—é‡è¦åº¦å›¾å’Œéƒ¨åˆ†ä¾èµ–æ€§å›¾æä¾›æ›´è¯¦ç»†çš„æ›¿ä»£æ–¹æ¡ˆã€‚

### 01 åŸç†å›é¡¾

![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071746722.png?imageSlim)
shapåŒ…ä¸­åŒ…å«äº†ä¸¤ç§å¯è§†åŒ–æ–¹æ¡ˆï¼Œåœ¨æ¦‚å¿µä¸Šå’Œå‰é¢æ‰€å­¦çš„permutation importance å’Œ partial dependence plotç±»ä¼¼

### 02 Summary Plotsï¼ˆç»™å‡ºæ¨¡å‹æ•´ä½“æ¦‚è§ˆï¼‰

	ç½®æ¢é‡è¦åº¦éå¸¸å¥½ï¼Œå› ä¸ºå®ƒåˆ›å»ºäº†ç®€å•çš„æ•°å­—åº¦é‡ï¼Œä»¥äº†è§£å“ªäº›ç‰¹å¾å¯¹æ¨¡å‹å¾ˆé‡è¦ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬è½»æ¾åœ°è¿›è¡Œç‰¹å¾ä¹‹é—´çš„æ¯”è¾ƒï¼Œè€Œä¸”æ‚¨è¿˜å¯ä»¥å‘éä¸“ä¸šè§‚ä¼—å±•ç¤ºç”±æ­¤ç”Ÿæˆçš„å›¾è¡¨ã€‚

	ä½†å®ƒå¹¶ä¸èƒ½å‘Šè¯‰æ‚¨æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ã€‚å¦‚æœæŸä¸ªç‰¹å¾çš„åŒ…ç»œé‡è¦æ€§å¤„äºä¸­ç­‰æ°´å¹³ï¼Œè¿™å¯èƒ½æ„å‘³ç€ï¼ˆ1ï¼‰å®ƒå¯¹å°‘æ•°é¢„æµ‹æœ‰è¾ƒå¤§å½±å“ï¼Œä½†æ€»ä½“ä¸Šæ²¡æœ‰å½±å“ï¼Œï¼ˆ2ï¼‰æˆ–å¯¹æ‰€æœ‰é¢„æµ‹éƒ½æœ‰ä¸­ç­‰å½±å“ã€‚

é€šè¿‡ SHAP æ±‡æ€»å›¾ï¼Œæˆ‘ä»¬å¯ä»¥é¸Ÿç°ç‰¹å¾é‡è¦æ€§åŠå…¶é©±åŠ¨å› ç´ ã€‚æˆ‘ä»¬å°†ä»¥è¶³çƒæ•°æ®ä¸ºä¾‹è¿›è¡Œåˆ†æï¼š

![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071749838.png?imageSlim)

==This plot is made of many dots. Each dot has three characteristics:==

- Vertical location shows what feature it is depictingå‚ç›´ä½ç½®æ˜¾ç¤ºæ‰€æè¿°çš„ç‰¹å¾
- Color shows whether that feature was high or low for that row of the dataseté¢œè‰²æ˜¾ç¤ºæ•°æ®é›†ä¸­è¯¥è¡Œçš„ç‰¹å¾å€¼æ˜¯å¤§è¿˜æ˜¯å°ï¼ˆæ¯ä¸€ä¸ªç‚¹ä»£è¡¨äº†æ•°æ®é›†ä¸­æŸä¸€è¡Œåœ¨è¯¥ç‰¹å¾ä¸Šçš„å–å€¼æ˜¯å¤§æ˜¯å°ï¼‰
- Horizontal location shows whether the effect of that value caused a higher or lower prediction.æ°´å¹³ä½ç½®æ˜¾ç¤ºè¯¥å€¼çš„å½±å“æ˜¯å¯¼è‡´é¢„æµ‹å€¼å‡é«˜è¿˜æ˜¯é™ä½ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºæŸä¸€ä¸ªç‚¹ï¼Œå¥¹æœ¬èº«åœ¨è¯¥ç‰¹å¾ä¸Šçš„å–å€¼ï¼ˆé¢œè‰²ï¼‰ï¼Œæœ€ç»ˆå¯¼è‡´é¢„æµ‹çš„å–å€¼å‡é«˜è¿˜æ˜¯é™ä½ï¼‰

For example, the point in the upper left was for a team that scored few goals, reducing the prediction by 0.25.

==Some things you should be able to easily pick out:==

- The model ignored theÂ `Red`Â andÂ `Yellow & Red`Â features.æ¨¡å‹å¿½ç•¥äº†çº¢ç‰Œå’Œé»„çº¢ç‰Œç‰¹å¾ã€‚
- UsuallyÂ `Yellow Card`Â doesn't affect the prediction, but there is an extreme case where a high value caused a much lower prediction.é€šå¸¸é»„ç‰Œä¸ä¼šå½±å“é¢„æµ‹ç»“æœï¼Œä½†æœ‰ä¸€ä¸ªæç«¯çš„ä¾‹å­ï¼Œé»„ç‰Œå€¼é«˜ä¼šå¯¼è‡´é¢„æµ‹ç»“æœå¤§å¤§é™ä½
- High values of Goal scored caused higher predictions, and low values caused low predictionsè¿›çƒæ•°çš„é«˜å€¼ä¼šå¯¼è‡´è¾ƒé«˜çš„é¢„æµ‹å€¼ï¼Œè€Œä½å€¼åˆ™ä¼šå¯¼è‡´è¾ƒä½çš„é¢„æµ‹å€¼ã€‚

If you look for long enough, there's a lot of information in this graph. You'll face some questions to test how you read them in the exercise.å¦‚æœä½ è§‚å¯Ÿçš„æ—¶é—´è¶³å¤Ÿé•¿ï¼Œè¿™å¼ å›¾ä¸­ä¼šæœ‰å¾ˆå¤šä¿¡æ¯ã€‚åœ¨ç»ƒä¹ ä¸­ï¼Œä½ ä¼šé‡åˆ°ä¸€äº›é—®é¢˜æ¥æµ‹è¯•ä½ æ˜¯å¦‚ä½•è¯»æ‡‚å®ƒä»¬çš„ã€‚

==ä»£ç å¦‚ä¸‹ï¼š==

	å¯¼å…¥æ•°æ®é›†
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071800637.png?imageSlim)

```python
import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# calculate shap values. This is what we will plot.
# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values = explainer.shap_values(val_X)

# Make plot. Index of [1] is explained in text below.
shap.summary_plot(shap_values[1], val_X)
```

![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071802565.png?imageSlim)

The code isn't too complex. But there are a few caveats.

- When plotting, we callÂ `shap_values[1]`. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of "True".ç»˜å›¾æ—¶ï¼Œæˆ‘ä»¬è°ƒç”¨ shap_values[1]ã€‚å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œæ¯ä¸ªå¯èƒ½çš„ç»“æœéƒ½æœ‰ä¸€ä¸ªå•ç‹¬çš„ SHAP å€¼æ•°ç»„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šè¿‡ç´¢å¼•æ¥è·å–==é¢„æµ‹ç»“æœä¸º "çœŸ "çš„ SHAP å€¼==ã€‚
- Calculating SHAP values can be slow. It isn't a problem here, because this dataset is small. But you'll want to be careful when running these to plot with reasonably sized datasets. The exception is when using anÂ `xgboost`Â model, which SHAP has some optimizations for and which is thus much faster.è®¡ç®— SHAP å€¼å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ã€‚ç”±äºæ•°æ®é›†è¾ƒå°ï¼Œåœ¨è¿™é‡Œè¿™ä¸æ˜¯é—®é¢˜ã€‚ä½†åœ¨ä½¿ç”¨åˆç†å¤§å°çš„æ•°æ®é›†è¿›è¡Œç»˜å›¾æ—¶ï¼Œæ‚¨éœ€è¦å°å¿ƒè°¨æ…ã€‚ä½¿ç”¨ xgboost æ¨¡å‹æ˜¯ä¸ªä¾‹å¤–ï¼ŒSHAP å¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†ä¸€äº›ä¼˜åŒ–ï¼Œå› æ­¤é€Ÿåº¦è¦å¿«å¾—å¤šã€‚


	This provides a great overview of the model, but we might want to delve into a single feature. That's where SHAP dependence contribution plots come into play.è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„æ¨¡å‹æ¦‚è§ˆï¼Œä½†æˆ‘ä»¬å¯èƒ½æƒ³æ·±å…¥ç ”ç©¶æŸä¸ªç‰¹å¾ã€‚è¿™å°±æ˜¯ SHAP ä¾èµ–æ€§è´¡çŒ®å›¾å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚


### 03 SHAP Dependence Contribution Plotsï¼ˆæ·±å…¥ç ”ç©¶æŸä¸ªç‰¹å¾ï¼‰

We've previously used Partial Dependence Plots to show how a single feature impacts predictions. These are insightful and relevant for many real-world use cases. Plus, with a little effort, they can be explained to a non-technical audience.æˆ‘ä»¬ä¹‹å‰æ›¾ä½¿ç”¨éƒ¨åˆ†ä¾èµ–å›¾ï¼ˆPartial Dependence Plotsï¼‰æ¥å±•ç¤ºå•ä¸€ç‰¹å¾å¯¹é¢„æµ‹çš„å½±å“ã€‚è¿™äº›éƒ½å¾ˆæœ‰æ´å¯ŸåŠ›ï¼Œè€Œä¸”ä¸ç°å®ä¸–ç•Œä¸­çš„è®¸å¤šç”¨ä¾‹æ¯æ¯ç›¸å…³ã€‚æ­¤å¤–ï¼Œåªéœ€ç¨åŠ åŠªåŠ›ï¼Œæˆ‘ä»¬å°±èƒ½å‘éæŠ€æœ¯äººå‘˜è§£é‡Šå®ƒä»¬ã€‚

==But there's a lot they don't show. For instance, what is the distribution of effects? Is the effect of having a certain value pretty constant, or does it vary a lot depending on the values of other feaures. SHAP dependence contribution plots provide a similar insight to PDP's, but they add a lot more detail.ä½†æ˜¯ï¼Œå®ƒä»¬ä¹Ÿæœ‰å¾ˆå¤šæ²¡æœ‰æ˜¾ç¤ºçš„åœ°æ–¹ã€‚ä¾‹å¦‚ï¼Œæ•ˆæœçš„åˆ†å¸ƒæ˜¯æ€æ ·çš„ï¼Ÿå…·æœ‰æŸä¸ªå€¼çš„æ•ˆæœæ˜¯éå¸¸æ’å®šçš„ï¼Œè¿˜æ˜¯ä¼šå› å…¶ä»–ç‰¹å¾å€¼çš„ä¸åŒè€Œå˜åŒ–å¾ˆå¤§ã€‚SHAP ä¾èµ–æ€§è´¡çŒ®å›¾æä¾›äº†ä¸ PDP ç±»ä¼¼çš„æ´å¯ŸåŠ›ï¼Œä½†å®ƒä»¬å¢åŠ äº†æ›´å¤šç»†èŠ‚ã€‚==
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071807531.png?imageSlim)
Start by focusing on the shape, and we'll come back to color in a minute. Each dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes upward says that the more you possess the ball, the higher the model's prediction is for winning the Man of the Match award.é¦–å…ˆ==å…³æ³¨å½¢çŠ¶==ï¼Œç¨åæˆ‘ä»¬å†æ¥çœ‹é¢œè‰²ã€‚æ¯ä¸ªç‚¹ä»£è¡¨ä¸€è¡Œæ•°æ®ã€‚==æ°´å¹³ä½ç½®æ˜¯æ•°æ®é›†ä¸­çš„å®é™…å€¼ï¼Œå‚ç›´ä½ç½®è¡¨ç¤ºæ‹¥æœ‰è¯¥å€¼å¯¹é¢„æµ‹çš„å½±å“ã€‚==è¿™ä¸ªå‘ä¸Šå€¾æ–œçš„äº‹å®è¯´æ˜ï¼Œä½ æ§çƒè¶Šå¤šï¼Œæ¨¡å‹å¯¹èµ¢å¾—æ¯”èµ›å…ˆç”Ÿå¥–çš„é¢„æµ‹å°±è¶Šé«˜ã€‚

The spread suggests that other features must interact with Ball Possession %. For example, here we have highlighted two points with similar ball possession values. That value caused one prediction to increase, and it caused the other prediction to decrease.è¿™ç§åˆ†å¸ƒè¡¨æ˜ï¼Œ==å…¶ä»–ç‰¹å¾å¯èƒ½ä¸æ§çƒç‡ç›¸äº’ä½œç”¨==ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬çªå‡ºæ˜¾ç¤ºäº†ä¸¤ä¸ªæ§çƒç‡å€¼ç›¸ä¼¼çš„ç‚¹ã€‚è¯¥å€¼å¯¼è‡´ä¸€ä¸ªé¢„æµ‹å€¼ä¸Šå‡ï¼Œè€Œå¦ä¸€ä¸ªé¢„æµ‹å€¼ä¸‹é™ã€‚
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071811664.png?imageSlim)

For comparison, a simple linear regression would produce plots that are perfect lines, without this spread.ç›¸æ¯”ä¹‹ä¸‹ï¼Œç®€å•çš„çº¿æ€§å›å½’ç»˜åˆ¶å‡ºçš„æ›²çº¿å›¾æ˜¯å®Œç¾çš„ç›´çº¿ï¼Œæ²¡æœ‰è¿™ç§å·®å¼‚ã€‚ï¼ˆå¦‚æœæ˜¯ç‰¹å¾æ˜¯çº¿æ€§å›å½’çš„ï¼Œå°±ä¸ä¼šå‡ºç°è¿™ç§çš„å·®å¼‚ï¼‰

This suggests we delve into the ==interactions==, and the plots include color coding to help do that. While the primary trend is upward, you can visually inspect whether that varies by dot color.è¿™å°±éœ€è¦æˆ‘ä»¬æ·±å…¥ç ”ç©¶äº¤äº’ä½œç”¨ï¼Œå›¾ä¸­çš„é¢œè‰²ç¼–ç å¯ä»¥å¸®åŠ©æˆ‘ä»¬åšåˆ°è¿™ä¸€ç‚¹ã€‚è™½ç„¶ä¸»è¦è¶‹åŠ¿æ˜¯å‘ä¸Šçš„ï¼Œä½†æ‚¨å¯ä»¥==é€šè¿‡ç‚¹çš„é¢œè‰²==ç›´è§‚åœ°æŸ¥çœ‹æ˜¯å¦æœ‰å˜åŒ–ã€‚ï¼ˆçœ‹å®Œå½¢çŠ¶çœ‹é¢œè‰²ï¼‰

Consider the following very narrow example for concreteness.è¯·çœ‹ä¸‹é¢è¿™ä¸ªéå¸¸ç‹­çª„çš„ä¾‹å­ã€‚
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071813514.png?imageSlim)
These two points stand out spatially as being far away from the upward trend. They are both colored purple, indicating the team scored one goal. You can interpret this to sayÂ **In general, having the ball increases a team's chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.**
ä»ç©ºé—´ä¸Šçœ‹ï¼Œè¿™ä¸¤ç‚¹ä¸ä¸Šå‡è¶‹åŠ¿ç›¸è·ç”šè¿œã€‚å®ƒä»¬éƒ½è¢«æŸ“æˆç´«è‰²ï¼Œè¡¨ç¤ºè¯¥é˜Ÿè¿›äº†ä¸€çƒã€‚æ‚¨å¯ä»¥å°†å…¶ç†è§£ä¸ºâ€”â€”==ä¸€èˆ¬æ¥è¯´ï¼Œæ‹¥æœ‰çƒæƒä¼šå¢åŠ çƒé˜Ÿçƒå‘˜è·å¥–çš„æœºä¼šï¼ˆä»å€¾æ–œçš„å½¢çŠ¶çœ‹å‡ºæ¥çš„ï¼‰==ã€‚ä½†å¦‚æœä»–ä»¬åªè¿›äº†ä¸€ä¸ªçƒï¼Œè¿™ä¸€è¶‹åŠ¿å°±ä¼šé€†è½¬ï¼Œå¦‚æœä»–ä»¬åªè¿›äº†é‚£ä¹ˆä¸€ç‚¹ç‚¹çƒï¼Œè¯„å§”ä»¬å¯èƒ½ä¼šå› ä¸ºä»–ä»¬ç»å¸¸æ§çƒè€Œæƒ©ç½šä»–ä»¬ã€‚

==Outside of those few outliers, the interaction indicated by color isn't very dramatic here. But sometimes it will jump out at you.é™¤äº†è¿™å‡ ä¸ªå¼‚å¸¸å€¼ä¹‹å¤–ï¼Œç”¨é¢œè‰²è¡¨ç¤ºçš„äº¤äº’ä½œç”¨åœ¨è¿™é‡Œå¹¶ä¸ååˆ†æ˜¾è‘—ã€‚ä½†æœ‰æ—¶å®ƒä¼šè®©ä½ çœ¼å‰ä¸€äº®ã€‚==


==ä»£ç ==
	We get the dependence contribution plot with the following code. The only line that's different from theÂ `summary_plot`Â is the last line.

```python
import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# calculate shap values. This is what we will plot.
shap_values = explainer.shap_values(X)

# make plot.
shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index="Goal Scored")
```
![image.png](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403071820542.png?imageSlim)

If you don't supply an argument forÂ `interaction_index`, Shapley uses some logic to pick one that may be interesting.å¦‚æœä½ æ²¡æœ‰åˆ›é€ interaction_indexçš„æƒ³æ³•ï¼ŒShapleyä¼šä½¿ç”¨ä¸€äº›é€»è¾‘æ¥é€‰æ‹©ä¸€ä¸ªå¯èƒ½æœ‰è¶£çš„è®ºæ®ã€‚

è¿™ä¸éœ€è¦ç¼–å†™å¤§é‡ä»£ç ã€‚ä½†è¿™äº›æŠ€æœ¯çš„è¯€çªåœ¨äºæ‰¹åˆ¤æ€§åœ°æ€è€ƒç»“æœï¼Œè€Œä¸æ˜¯ç¼–å†™ä»£ç æœ¬èº«ã€‚

==æ˜¯è°è¿™ä¹ˆå¤©æ‰==