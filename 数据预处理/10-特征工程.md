[Creating Features | Kaggle](https://www.kaggle.com/code/ryanholbrook/creating-features)


### 1、特征创造

	了解特征。参考数据集的数据文档（如果有的话）。
	研究问题领域，获取领域知识。如果您的问题是预测房价，那么可以做一些房地产方面的研究。维基百科可能是一个很好的起点，但书籍和期刊论文通常能提供最好的信息。
	研究以前的工作。过去 Kaggle 竞赛中的解决方案文章是很好的资源。
	使用数据可视化。可视化可以揭示特征分布中的病理或可以简化的复杂关系。在特征工程设计过程中，一定要将数据集可视化。


#### **数字特征之间的公式变换**

数字特征之间的关系通常通过数学公式来表达，这是您在**领域研究中经常会遇到的**。在 Pandas 中，您可以对列进行算术运算，就像对普通数字一样。

汽车数据集中有描述汽车发动机的特征。通过研究可以获得各种公式，用于创建可能有用的新特征。例如，"冲程比 "可以衡量发动机的效率和性能：

Data visualization can suggest transformations, often a "reshaping" of a feature through powers or logarithms. The distribution of `WindSpeed` in _US Accidents_ is ==highly skewed, for instance. In this case the logarithm is effective at normalizing it:==

```python
# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log
accidents["LogWindSpeed"] = accidents.WindSpeed.apply(np.log1p)

# Plot a comparison
fig, axs = plt.subplots(1, 2, figsize=(8, 4))
sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])
sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);
```


##### Numerical Transformations[](https://www.kaggle.com/code/arunklenin/ps4e1-advanced-feature-engineering-ensemble#4.1-Numerical-Transformations)

We're going to see what transformation works better for each feature and select them, the idea is to compress the data. There could be situations where you will have to stretch the data. These are the methods applied:

1. **Log Transformation**: This transformation involves taking the logarithm of each data point. It is ==useful when the data is highly skewed and the variance increases with the mean.当数据高度倾斜且方差随平均值增加时，这种方法非常有用。==
    
    ```
             y = log(x)
    ```
    
2. **Square Root Transformation**: This transformation involves taking the square root of each data point. It is ==useful when the data is highly skewed and the variance increases with the mean.==
    
    ```
             y = sqrt(x)
    ```
    
3. **Box-Cox Transformation**: This transformation is a family of power transformations that includes the log and square root transformations as special cases. It is useful when the data is highly skewed and the variance increases with the mean.此变换是一系列幂变换，其中包括对数和平方根变换作为特殊情况。当数据高度偏斜且方差随均值增加时，它非常有用。
    
    ```
             y = [(x^lambda) - 1] / lambda if lambda != 0
             y = log(x) if lambda = 0
    ```
    
4. **Yeo-Johnson Transformation**: This transformation is similar to the Box-Cox transformation, but it can be applied to both positive and negative values. It is useful when the data is highly skewed and the variance increases with the mean.此变换类似于 Box-Cox 变换，但它可以应用于正值和负值。当数据高度偏斜且方差随均值增加时，它非常有用。

```python
#Apply PowerTransformer for a normalized like distribution
from sklearn.preprocessing import PowerTransformer
num_train3 = num_train.copy()

transformer = PowerTransformer(method = 'yeo-johnson')

num_train3 = transformer.fit_transform(num_train3)

num_train3 = pd.DataFrame(num_train3, columns = num_train.columns)
num_train3
```
![Pasted image 20240229145500](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854898.png?imageSlim)

    
    ```
             y = [(|x|^lambda) - 1] / lambda if x >= 0, lambda != 0
             y = log(|x|) if x >= 0, lambda = 0
             y = -[(|x|^lambda) - 1] / lambda if x < 0, lambda != 2
             y = -log(|x|) if x < 0, lambda = 2
    ```
	
5. **Power Transformation**: This transformation involves raising each data point to a power. It is useful when the data is highly skewed and the variance increases with the mean. The power can be any value, and is often determined using statistical methods such as the Box-Cox or Yeo-Johnson transformations.这种转换涉及将每个数据点提升到一个幂。当数据高度偏斜且方差随均值增加时，它非常有用。功效可以是任何值，通常使用统计方法（如 Box-Cox 或 Yeo-Johnson 变换）确定。
    
    ```
             y = [(x^lambda) - 1] / lambda if method = "box-cox" and lambda != 0
             y = log(x) if method = "box-cox" and lambda = 0
             y = [(x + 1)^lambda - 1] / lambda if method = "yeo-johnson" and x >= 0, lambda != 0
             y = log(x + 1) if method = "yeo-johnson" and x >= 0, lambda = 0
             y = [-(|x| + 1)^lambda - 1] / lambda if method = "yeo-johnson" and x < 0, lambda != 2
             y = -log(|x| + 1) if method = "yeo-johnson" and x < 0, lambda = 2
    ```


#### 计数
	描述某种事物存在或不存在的特征往往是成套的，例如某种疾病的风险因素集。您可以通过创建计数来汇总这些特征。
	
	这些特征将是二进制（1 表示存在，0 表示不存在）或布尔型（真或假）。在 Python 中，布尔值可以像整数一样相加。
	
	在 "交通事故"（Traffic Accidents）中，有几个特征表示事故附近是否有道路物体。这将**使用求和方法创建附近道路特征总数的计数**：

For each categorical/discrete variable, perform the following encoding techniques:

- **Count/Frequency Encoding**: Count the number of occurrences of each category and replace the category with its count.
- **Count Labeling**: Assign a label to each category based on its count, with higher counts receiving higher labels.
- **Target-Guided Mean Encoding**: Rank the categories based on the mean of target column across each category
- **One-Hot Encoding**: Apply OHE if the unique value is less than N (avoid creating so many features)

Please note that a particular encoding technique is not selected only if it has superior technique and the correlation with that is high
#### 建立/拆解特征

	建立和分解功能
	通常情况下，复杂的字符串可以分解成较简单的部分。一些常见的例子
	
	ID 号码： '123-45-6789'
	电话号码： '(999) 555-0123'
	街道地址："8241 Kaggle Ln., Goose City, NV
	互联网地址：'http://www.kaggle.com
	产品代码："0 36000 29145 2
	日期和时间："Mon Sep 30 07:06:05 2013
	类似这样的功能通常都有某种结构，您可以加以利用。例如，美国电话号码有一个区号（"(999) "部分），可以告诉您来电者的位置。和往常一样，在这里进行一些研究会有所收获。
	
	使用 str 访问器，可以直接将 split 等字符串方法应用到列中。客户终身价值数据集包含描述保险公司客户的特征。我们可以从保单特征中分离出类型和承保级别：

#### 成组变换

	最后，我们还有分组转换，它可以汇总按某个类别分组的多行信息。通过组转换，你可以创建以下功能： "一个人居住州的平均收入 "或 "按类型划分的工作日上映电影的比例"。如果你发现了一个类别的交互作用category interaction，那么对该类别进行分组变换可能是一个很好的研究方向。

##### 1. 根据groupby+内置分组特征产生新变量的方法

	使用聚合函数，分组变换结合了两个特征：一个是提供分组的分类特征，另一个是您希望聚合其值的特征。对于 "各州平均收入"，您可以选择州作为分组特征，平均值作为聚合函数，收入作为聚合特征。要在 Pandas 中计算，我们需要使用 groupby 和 transform 方法：
	
	均值函数是一种内置的数据帧方法，这意味着我们可以将其作为字符串传递给转换器。其他方便的方法包括 max、min、median、var、std 和 count。下面是计算数据集中每种状态出现频率(连续性)的方法：
	The `mean` function is a built-in dataframe method, which means we can pass it as a string to `transform`. Other handy methods include `max`, `min`, `median`, `var`, `std`, and `count`. Here's how you could calculate the frequency with which each state occurs in the dataset:

```python
customer["AverageIncome"] = (
    customer.groupby("State")  # for each state
    ["Income"]                 # select the income
    .transform("mean")         # and compute its mean
)

customer[["State", "Income", "AverageIncome"]].head(10)
```

	您可以使用这样的转换为**分类特征**创建 "频率编码"。
	
	如果要使用训练集和验证集，为了保持它们的独立性，最好只使用训练集创建分组特征，然后将其与验证集合并。我们可以在训练集上使用 drop_duplicates 创建一组唯一值后，使用验证集的合并方法：
	
	You could use a transform like this to create a "frequency encoding" for a categorical feature.

If you're using training and validation splits, to preserve their independence, it's best to create a grouped feature using only the training set and then join it to the validation set. We can use the validation set's `merge` method after creating a unique set of values with `drop_duplicates` on the training set:
	
![Pasted image 20240229091717](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854899.png?imageSlim)
	
##### 2. X属性间interaction的识别和处理方法

在是否为活跃用户的判断中：X属性中的性别和历史购买数量之间存在interaction，因此用特征融合的方法

```python
class TpGenderTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, total_products_field='Total_Products_Used', gender_field='Gender'):
        self.total_products_field = total_products_field
        self.gender_field = gender_field
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X_copy = X.copy()
        X_copy['Tp_Gender'] = X_copy[self.total_products_field].astype('str') + X_copy[self.gender_field]
        return X_copy

train_tp = TotalProductsTransformer().fit_transform(train)   
TpGenderTransformer().fit_transform(train_tp)
```
![Pasted image 20240229164013](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854900.png?imageSlim)




创建特征的提示
	
	在创建功能时，最好牢记模型自身的优缺点。以下是一些指导原则：
	线性模型可以自然地学习和与差，但无法学习更复杂的内容。
	对于大多数模型来说，比率似乎很难学习。比率组合通常能轻松提高性能。
	线性模型和神经网络通常在使用归一化特征时表现更好。基于树的模型（如随机森林和 XGBoost）有时也能从归一化中获益，但通常要少得多。
	树状模型可以学习近似几乎所有的特征组合，但当某个组合特别重要时，它们仍然可以从明确创建该组合中获益，尤其是在数据有限的情况下。
	计数对树状模型尤其有帮助，因为这些模型没有一种自然的方法来同时汇总许多特征的信息。
	
**Tips on Creating Features**  
It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:

- Linear models learn sums and differences naturally, but can't learn anything more complex.
- ==**Ratios seem to be difficult for most models to learn**.== Ratio combinations often lead to some easy performance gains.
- Linear models and neural nets generally do better with **normalized features**. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.
- ==**Tree models can learn to approximate almost any combination of features**,== but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.
- **==Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.==**


### 2、 Clustering With K-Means
[Clustering With K-Means | Kaggle](https://www.kaggle.com/code/ryanholbrook/clustering-with-k-means)

利用集群标签解决复杂的空间关系。

This lesson and the next make use of what are known as _unsupervised learning_ algorithms. Unsupervised algorithms don't make use of a target; instead, their purpose is to l**earn some property of the data**, to represent the structure of the features in a certain way. ==**In the context of feature engineering for prediction, you could think of an unsupervised algorithm as a "feature discovery" technique.**==
==**在用于预测的特征工程中，你可以将无监督算法视为一种 "特征发现 "技术。**==

**Clustering** simply means the assigning of data points to groups based upon how similar the points are to each other. A clustering algorithm makes "birds of a feather flock together," so to speak.

When used for feature engineering, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. ==**Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity.添加聚类标签特征可以帮助机器学习模型理清复杂的空间或邻近关系。****
**==

作为特征的聚类标签

	对于一维实值特征，聚类的作用类似于传统的 "分箱 "或 "离散化 "变换。对于多个特征，它就像 "多维分选"（有时也称为向量量化）。
Applied to a single real-valued feature, clustering acts like a traditional "binning" or ["discretization"](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html) transform. On multiple features, it's like "multi-dimensional binning" (sometimes called _vector quantization_).
![Pasted image 20240229093937](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854901.png?imageSlim)

	重要的是要记住，这个聚类特征是分类特征。这里显示的是典型聚类算法会产生的标签编码（即整数序列）；取决于您的模型，独热编码可能更合适。
	
	添加聚类标签的动机是，聚类将把特征间的复杂关系分解成更简单的块。这样，我们的模型就可以逐一学习较简单的部分，而不必一次性学习复杂的整体。这是一种 "分而治之 "的策略。

![Pasted image 20240229094153](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854902.png?imageSlim)
	
	该图显示了聚类如何改进简单的线性模型。对于这种模型来说，"建造年份 "和 "销售价格 "之间的曲线关系过于复杂--它的拟合效果不佳。然而，在较小的块上，这种关系几乎是线性的，因此模型可以很容易地学习。

#### k 均值聚类
	聚类算法有很多。它们的主要区别在于如何衡量 "相似性 "或 "接近性"，以及使用何种特征。我们将使用的 k-means 算法非常直观，易于在特征工程中应用。取决于你的应用情况，其他算法可能更适合你。

	【数据集量大】如果数据集数量较多，可能需要增加 max_iter，如果数据集较复杂，可能需要增加 n_init。但通常情况下，你唯一需要自己选择的参数是 n_clusters（即 k）。一组特征的最佳划分取决于你正在使用的模型和你试图预测的内容，因此最好像调整其他超参数一样调整它（比如通过交叉验证）。

	【kmeans需要计算距离】由于 k-means 聚类对尺度很敏感，因此最好对具有极端值的数据进行重新缩放或归一化。我们的特征已经大致在同一尺度上，因此我们将保持不变

![Pasted image 20240229094827](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854903.png?imageSlim)

	Since k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values. Our features are already roughly on the same scale, so we'll leave them as-is.
	

![Pasted image 20240229094847](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854904.png?imageSlim)

![Pasted image 20240229094921](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854905.png?imageSlim)

	该数据集中的目标变量是 MedHouseVal（房屋价值中位数）。这些方框图显示了目标值在每个聚类中的分布情况。如果聚类是infomative的，那么这些分布在 MedHouseVal 上大部分应该是分开的，这正是我们所看到的。

![Pasted image 20240229095047](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854907.png?imageSlim)


### 3、PCA
	Discover new features by analyzing variation.

	主成分分析（PCA）。就像聚类是基于接近性对数据集进行划分一样，你也可以将 PCA 视为对数据中的变化进行划分。PCA 是帮助您发现数据中重要关系的绝佳工具，也可用于创建信息量更大的特征。
	
	(技术说明：PCA 通常应用于标准化数据。对于标准化数据，"变异 "意味着 "相关性"。对于非标准化数据，"变异 "指的是 "协方差"。本课程中的所有数据在应用 PCA 之前都将标准化）。
(Technical note: PCA is typically applied to [standardized](https://www.kaggle.com/alexisbcook/scaling-and-normalization) data. With standardized data "variation" means "correlation". With unstandardized data "variation" means "covariance". All data in this course will be standardized before applying PCA.)

	鲍鱼数据集中有几千个塔斯马尼亚鲍鱼的物理测量值。(鲍鱼是一种很像蛤蜊或牡蛎的海洋生物）。我们现在只看几个特征：鲍鱼壳的 "高度 "和 "直径"。
	
	您可以想象，在这些数据中，有一些 "变异轴 "描述了鲍鱼之间的差异。从图像上看，这些轴线是沿着数据的自然维度运行的垂直线，每个原始特征有一个轴线。

![Pasted image 20240229100659](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854908.png?imageSlim)

	通常，我们可以给这些变异轴命名。较长的轴线我们可以称为 "尺寸 "部分：小高度和小直径（左下）与大高度和大直径（右上）形成对比。较短的轴线我们可以称为 "形状 "部分：小高度和大直径（扁平形状）与大高度和小直径（圆形）形成对比。
	
	请注意，与其用 "高度 "和 "直径 "来描述鲍鱼，不如用 "大小 "和 "形状 "来描述鲍鱼。事实上，这就是 PCA 的整个理念：我们不是用原始特征来描述数据，而是用数据的变异轴来描述它。变异轴就是新的特征。

![Pasted image 20240229100906](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854909.png?imageSlim)

The new features PCA constructs are actually ==just linear combinations== (weighted sums) of the original features:

![Pasted image 20240229101136](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854910.png?imageSlim)

	这张载荷表告诉我们，在 "尺寸 "成分中，"高度 "和 "直径 "的变化方向相同（符号相同），但在 "形状 "成分中，它们的变化方向相反（符号相反）。在每个分量中，载荷的大小都相同，因此特征在两个分量中的作用相同。
	
	PCA 还能告诉我们每个成分的变化量。从图中我们可以看出，数据在 "尺寸 "分量上的变化比在 "形状 "分量上的变化要大。PCA 通过每个分量的解释方差百分比精确地说明了这一点。
	
![Pasted image 20240229101241](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854911.png?imageSlim)
	
	The `Size` component captures the majority of the variation between `Height` and `Diameter`. It's important to remember, however, that the amount of variance in a component doesn't necessarily correspond to how good it is as a predictor: it depends on what you're trying to predict
尺寸 "成分捕捉了 "身高 "和 "直径 "之间的大部分变异。不过，重要的是要记住，==一个成分的变异量并不一定与它作为预测指标的好坏相对应：这取决于你试图预测什么。==

##### PCA for Feature Engineering

有两种方法可以将 PCA 用于特征工程。

	The first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product of `'Height'` and `'Diameter'` if `'Size'` is important, say, or a ratio of `'Height'` and `'Diameter'` if `Shape` is important. You could even try clustering on one or more of the high-scoring components.
第一种方法是将其用作描述性技术。==由于component可以说明变异情况，因此可以计算component的 MI 分数，看看哪种变异对目标变量最有预测性。这可以为您提供创建特征类型的思路==--例如，如果 "尺寸 "很重要，可以创建 "高度 "和 "直径 "的乘积；如果 "形状 "很重要，可以创建 "高度 "和 "直径 "的比率。您甚至可以尝试对一个或多个高分成分进行聚类。

第二种方法是使用成分本身作为特征。由于成分直接暴露了数据的变异结构，因此它们往往比原始特征更有信息量。下面是一些使用案例：

- **Dimensionality reduction**: When your features are highly redundant (_multicollinear_, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.
- **Anomaly detection**: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.
- **Noise reduction**: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.
- **Decorrelation**: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.

- 降维： 当特征高度冗余（特别是多重共线性特征）时，PCA 会将冗余划分为一个或多个方差接近零的分量，由于这些分量几乎不包含任何信息，因此可以将其丢弃。
- 异常检测： 在原始特征中并不明显的异常变化往往会出现在低方差成分中。==在异常或离群点检测任务中，这些成分可能具有很高的信息量。==
- 降低噪音： ==传感器读数==集合通常会有一些共同的背景噪声。PCA 有时可以将（信息量大的）信号收集到数量较少的特征中，而将噪声排除在外，从而提高信噪比。
- 去相关性： 有些多重特征法在处理高度相关的特征时会遇到困难。PCA 可将相关特征转化为不相关的成分，从而使算法更容易处理。

应用 PCA 时需要注意以下几点：
PCA 只适用于数字特征，如连续数量或计数。
PCA 对规模很敏感。在应用 PCA 之前，最好先将数据标准化，除非您有充分的理由不这样做。
考虑移除或限制异常值，因为它们会对结果产生不当影响。


![Pasted image 20240229104225](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854912.png?imageSlim)

```python
## MI和plot_variance
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from IPython.display import display
from sklearn.feature_selection import mutual_info_regression

plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)

def plot_variance(pca, width=8, dpi=100):
    # Create figure
    fig, axs = plt.subplots(1, 2)
    n = pca.n_components_
    grid = np.arange(1, n + 1)
    # Explained variance
    evr = pca.explained_variance_ratio_
    axs[0].bar(grid, evr)
    axs[0].set(
        xlabel="Component", title="% Explained Variance", ylim=(0.0, 1.0)
    )
    # Cumulative Variance
    cv = np.cumsum(evr)
    axs[1].plot(np.r_[0, grid], np.r_[0, cv], "o-")
    axs[1].set(
        xlabel="Component", title="% Cumulative Variance", ylim=(0.0, 1.0)
    )
    # Set up figure
    fig.set(figwidth=8, dpi=100)
    return axs

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

df = pd.read_csv("../input/fe-course-data/autos.csv")
```

![Pasted image 20240229104246](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854913.png?imageSlim)

	拟合后，PCA 实例的 components_ 属性中就包含了载荷。(不幸的是，PCA 的术语并不一致。我们按照惯例将 X_pca 中的转换列称为成分，否则它们就没有名称）。我们将用一个数据帧来封装载荷。

![Pasted image 20240229104554](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854914.png?imageSlim)

回想一下，==一个component's loadings的符号和大小可以告诉我们它捕捉到了哪种变化==。第一个分量（PC1）显示了油耗较低的大型、动力强劲的汽车与油耗较高的小型、更经济的汽车之间的对比。我们可以称之为 "豪华/经济 "轴。下图显示，我们选择的四个特征大多沿 "豪华/经济 "轴变化。

![Pasted image 20240229105005](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854915.png?imageSlim)

	第三个部分显示了马力和整备质量之间的不和谐 -- 似乎是跑车和旅行车的对比。

![Pasted image 20240229105225](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854916.png?imageSlim)
![Pasted image 20240229105344](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854917.png?imageSlim)
![Pasted image 20240229105411](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854918.png?imageSlim)





### 4、Target Encoding
[[01-编码]]
	Boost any categorical feature with this powerful technique.
	利用这一强大的技术提升任何分类特征。

	我们在本课程中看到的大多数技术都是针对数字特征的。本课我们要学习的目标编码技术则是针对分类特征的。它是一种将类别编码为数字的方法，就像单独热编码或label编码，不同之处在于它还使用目标来创建编码。这就是我们所说的监督特征工程技术。

目标编码是任何一种将特征类别替换为目标数字的编码方式。

	一个简单有效的方法是应用第 3 课中的分组聚合，比如平均值。使用 "汽车 "数据集，可以计算出每种车型的平均价格：

![Pasted image 20240229110353](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854919.png?imageSlim)

This kind of target encoding is ==sometimes called a mean encoding. Applied to a binary target, it's also called bin counting==. (Other names you might come across include: likelihood encoding, impact encoding, and leave-one-out encoding.)

#### Smoothing平滑化 

然而，这样的编码会带来一些问题。

首先是_unknown categories_。目标编码会产生过拟合的特殊风险，这意味着需要在独立的 "编码 "分割上对其进行训练。当您将编码连接到未来的拆分时，Pandas 会为编码拆分中不存在的任何类别填补缺失值When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split。这些缺失值需要以某种方式进行估算。

其次是罕见类别。当一个类别在数据集中只出现几次时，对其分组计算的任何统计数据都不可能非常准确。在 "汽车 "数据集中，"Mercurcy "只出现过一次。我们计算出的 "平均 "价格只是这一辆车的价格，可能并不能完全代表我们未来可能看到的任何 Mercury。对稀有类别进行目标编码会增加过度拟合的可能性。

==解决这些问题的方法是增加平滑处理。我们的想法是将类别内的平均值与整体平均值混合起来。稀有类别在其类别平均值上的权重较低，而缺失类别则只获得整体平均值。==

用伪代码表示![Pasted image 20240229110757](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854920.png?imageSlim)
其中 n 是该类别在数据中出现的总次数。参数 m 决定了 "平滑系数"。m 值越大，总体估计值的权重越大。

When choosing a value for `m`, ==consider how noisy you expect the categories to be. Does the price of a vehicle vary a great deal within each make? Would you need a lot of data to get good estimates?== If so, it could be better to choose a larger value for `m`; if the average price for each make were relatively stable, a smaller value could be okay.


**Use Cases for Target Encoding**  
Target encoding is great for:

- **High-cardinality features**: ==A feature with a large number of categories== can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.
- **Domain-motivated features**: From prior experience, ==you might suspect that a categorical feature should be important== even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness.

##### Example

电影中的ratings是预测变量，zipcode包含3000类

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import warnings

plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
warnings.filterwarnings('ignore')
```

With over 3000 categories, the `Zipcode` feature makes a good candidate for target encoding, and the size of this dataset (over one-million rows) means we can spare some data to create the encoding.

==We'll start by creating a 25% split to train the target encoder.==

```python
## 这部分分了encode训练集
X = df.copy()
y = X.pop('Rating')

X_encode = X.sample(frac=0.25)
y_encode = y[X_encode.index]
X_pretrain = X.drop(X_encode.index)
y_train = y[X_pretrain.index]
```

The `category_encoders` package in `scikit-learn-contrib` ==implements an m-estimate encoder==, which we'll use to encode our `Zipcode` feature.

```python
from category_encoders import MEstimateEncoder

# Create the encoder instance. Choose m to control noise.
encoder = MEstimateEncoder(cols=["Zipcode"], m=5.0)

# Fit the encoder on the encoding split.
encoder.fit(X_encode, y_encode)

# Encode the Zipcode column to create the final training data
X_train = encoder.transform(X_pretrain)
```

Let's compare the encoded values to the target to see how informative our encoding might be.

![Pasted image 20240229133651](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040854921.png?imageSlim)
