#归一化

中心化 Zero-centered：让所有记录减去一个固定值，让数据平移到某一个位置

缩放处理Scale：通过除以一个固定值，将数据固定在某个范围中，取对数也算是一种缩放

[sklearn.preprocessing.normalize — scikit-learn 1.3.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html)

1. **What is Normalization（Max-Min Scaling）特指最大最小值归一化?**
	    按照最小值中心化之后，再按极差缩放，将数据收敛到0-1之间，归一化后数据服从正态分布
    - **Normalization** involves **scaling** the data to a common range. The goal is to transform the data so that it falls within a specific interval (often between 0 and 1 or -1 and 1).
    - Machine learning algorithms tend to perform better when features are on a smaller scale. Normalization helps achieve this by making the data more suitable for training.
2.  **按照均值和标准差进行缩放：Standnrdization /Z-Score normalization，数据标准化，使之成为0均值1方差的标准正态分布**


一般来说，大多数机器学习算法中会选择StandardScaler进行特征缩放，因为MinMaxScaler对异常值敏感。尤其PCA，聚类，Logistic回归、支持向量机、NN中，StandardScaler往往是最好的

MinMaxScaler在不涉及距离度量、梯度、协方差计算，以及数据需要被压缩到特定区间时应用广泛。
一般来说，先试试看标准化，效果不好再尝试归一化。

sklearn中还包含其他缩放处理：
希望压缩数据，但不影响数据稀疏性（不影响矩阵中取值为0的个数时），使用MaxAbsScaler；
在异常值多，噪声非常大时，可能会选用分位数来无量纲化：RobustScaler




**Data preprocessing** is a crucial step in preparing raw data for analysis, especially when working with machine learning or data mining algorithms. Let’s focus on **normalization**, which is one of the essential techniques in data preprocessing.

2. **Methods for Normalization:**
    
    - **Min-Max Normalization (Scaling):** Scales the data to a specified range (e.g., [0, 1]).
    - **Z-Score Normalization (Standardization):** Transforms the data to have a mean of 0 and a standard deviation of 1.
    - **Decimal Scaling:** Shifts the decimal point of the data values.
    - **L2 Normalization (Euclidean Norm):** Scales vectors individually to have a length of one.
3. **Using scikit-learn for Normalization:**
    
    - The `scikit-learn` library provides the `preprocessing.normalize()` function.
    - It scales vectors individually to a unit norm (default is L2 norm).
    - Example using a one-dimensional NumPy array:
        
        ```python
        import numpy as np
        from sklearn import preprocessing
        
        x_array = np.array([2, 3, 5, 6, 7, 4, 8, 7, 6])
        normalized_arr = preprocessing.normalize([x_array])
        print(normalized_arr)
        ```
        
        Output:
        
        ```
        [[0.11785113 0.1767767 0.29462783 0.35355339 0.41247896 0.23570226
          0.47140452 0.41247896 0.35355339]]
        ```
        
        The values are now in the range [0, 1], and their sum of squares is approximately 1.
4. **Normalizing Columns from a DataFrame:**
    
    - In a pandas DataFrame, you can normalize columns (features) using the same `preprocessing.normalize()` function.
    - Example using the California Housing dataset:
        
        ```python
        from sklearn import preprocessing
        import pandas as pd
        from sklearn.datasets import fetch_california_housing
        
        california_housing = fetch_california_housing(as_frame=True)
        scaler = preprocessing.MinMaxScaler(feature_range=(0, 2))
        d = scaler.fit_transform(california_housing.data)
        scaled_df = pd.DataFrame(d, columns=california_housing.data.columns)
        print(scaled_df)
        ```
        

Remember that the specific normalization method depends on your data and the problem you’re solving. Choose the one that best suits your needs! 📊🔍



--------------------------------------------------
**Data preprocessing** is a crucial step in preparing raw data for analysis, especially when working with machine learning or data mining algorithms. Let’s focus on **normalization**, which is one of the essential techniques in data preprocessing.

1. **What is Normalization?**
    
    - **Normalization** involves **scaling** the data to a common range. The goal is to transform the data so that it falls within a specific interval (often between 0 and 1 or -1 and 1).
    - Machine learning algorithms tend to perform better when features are on a smaller scale. Normalization helps achieve this by making the data more suitable for training.
2. **Methods for Normalization:**
    
    - **Min-Max Normalization (Scaling):** Scales the data to a specified range (e.g., [0, 1]).
    - **Z-Score Normalization (Standardization):** Transforms the data to have a mean of 0 and a standard deviation of 1.
    - **Decimal Scaling:** Shifts the decimal point of the data values.
    - **L2 Normalization (Euclidean Norm):** Scales vectors individually to have a length of one.
3. **Using scikit-learn for Normalization:**
    
    - The `scikit-learn` library provides the `preprocessing.normalize()` function.
    - It scales vectors individually to a unit norm (default is L2 norm).
    - Example using a one-dimensional NumPy array:
        
        ```python
        import numpy as np
        from sklearn import preprocessing
        
        x_array = np.array([2, 3, 5, 6, 7, 4, 8, 7, 6])
        normalized_arr = preprocessing.normalize([x_array])
        print(normalized_arr)
        ```
        
        Output:
        
        ```
        [[0.11785113 0.1767767 0.29462783 0.35355339 0.41247896 0.23570226
          0.47140452 0.41247896 0.35355339]]
        ```
        
        The values are now in the range [0, 1], and their sum of squares is approximately 1.
4. **Normalizing Columns from a DataFrame:**
    
    - In a pandas DataFrame, you can normalize columns (features) using the same `preprocessing.normalize()` function.
    - Example using the California Housing dataset:
        
        ```python
        from sklearn import preprocessing
        import pandas as pd
        from sklearn.datasets import fetch_california_housing
        
        california_housing = fetch_california_housing(as_frame=True)
        scaler = preprocessing.MinMaxScaler(feature_range=(0, 2))
        d = scaler.fit_transform(california_housing.data)
        scaled_df = pd.DataFrame(d, columns=california_housing.data.columns)
        print(scaled_df)
        ```
        

Remember that the specific normalization method depends on your data and the problem you’re solving. Choose the one that best suits your needs! 📊🔍