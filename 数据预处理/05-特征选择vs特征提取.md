### 特征选择方法综述
[scikit-learn中的特征选择方法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/141506312)
[如何为机器学习选择特征选择方法 - MachineLearningMastery.com](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)
[What Is Feature Engineering | Kaggle](https://www.kaggle.com/code/ryanholbrook/what-is-feature-engineering/tutorial)

### 1. 方差过滤

>适用对象：需要遍历特征或升维的算法
>主要目的：维持算法表现的前提下，帮助算法降低计算成本
>选取超参数threshold：每个数据集不一样，一定要选最优的超参数，可以画学习曲线，找模型最好的点，但现实往往不会这么做，因为非常耗时，只会使用阈值为0或者很小的方差来过滤，来率先消除一些明显用不到的特征，然后选择更优的特征选择方法继续削减特征数量

``` python
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold()
# 剔除零方差的属性
X_var0 = selector.fit_transform(X)

# 剔除方差小于中位数的属性
x_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
x_fsvar.shape
```
最近邻算法KNN、决策树、SVM、NN、回归算法，都需要便利特征或升维来进行运算，时间复杂度高，因此方差过滤的特征选择对他们尤其重要
但对于不需要遍历特征的算法，如rf，随机选取特征进行分支，本身运算就非常迅速，因此特征选择对他来说效果一般
——因为即使过滤法降低了特征的数量，随机森林也只会选取固定数量的特征来建模；而KNN对于更少的特征，模型明显会随着特征的减少而变得轻量

![Pasted image 20240130113448](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040853744.png?imageSlim)

方差过滤并不一定会使效果提升（看运气），主要是为了提升效率



### 2. 互信息

	在模型中添加一些基于领域背景知识的特征将有助于提升模型性能，但是识别这之中useful的属性用于combine是很重要的，因此可以使用Mutual Information来识别useful feature

	第一次遇到一个新的数据集有时会让人感到不知所措。您可能会看到成百上千个属性，甚至没有描述。你从哪里开始？

	一个很好的第一步是使用特征效用指标构建排名，该指标是衡量特征与目标之间关联的函数。然后，您可以选择一小部分最有用的功能进行初始开发，并更有信心您的时间将得到充分利用。

	我们将使用的指标称为“互信息”。互信息很像相关性，因为它测量两个量之间的关系。互信息的优点是它可以检测任何类型的关系，而相关性只能检测线性关系。

	互信息是一个很好的通用指标，在功能开发开始时特别有用，因为您可能还不知道要使用什么模型。互信息具有如下优点：
	易于使用和解释，
	计算效率高，
	理论上有根据，
	抗过拟合，以及
	能够检测任何类型的关系

	互信息中的不确定性是使用信息论中称为“熵”的量来衡量的。变量的熵大致意味着：“平均而言，您需要多少个是或否问题来描述该变量的出现。你要问的问题越多，你对变量的不确定性就越大。互信息是您希望该功能回答有关目标的问题数量。

	量之间可能的最小互信息是 0.0。当 MI 为零时，数量是独立的：两者都不能告诉你关于另一个的任何信息。相反，从理论上讲，MI可以是什么没有上限。然而，在实践中，高于 2.0 左右的值并不常见。（互信息是一个对数量，所以它增加得非常缓慢。

在应用互信息时，需要记住以下几点：

- MI can help you to understand the _relative potential_ of a feature as a predictor of the target, considered by itself.
- ==It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI _can't detect interactions_ between features. It is a **univariate** metric.==
- The _actual_ usefulness of a feature _depends on the model you use it with_. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn't mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.
特征的实际有用性取决于与之配合使用的模型。只有当特征与目标之间的关系是模型可以学习的关系时，特征才是有用的。一个特征的 MI 分数很高，但这并不意味着你的模型可以利用这些信息做任何事情。您可能需要先转换特征，以揭示关联。


	The scikit-learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that _must_ have a `float` dtype is _not_ discrete. Categoricals (`object` or `categorial` dtype) can be treated as discrete by giving them a label encoding. (You can review label encodings in our [Categorical Variables](http://www.kaggle.com/alexisbcook/categorical-variables) lesson.)
	
用于 MI 的 scikit-learn 算法对离散特征和连续特征的处理方式不同。因此，你需要告诉它哪些是离散特征。根据经验，任何必须使用浮点类型的特征都不是离散特征。分类（对象或分类 dtype）可以通过赋予标签编码来视为离散特征。(您可以在我们的分类变量课程中复习标签编码）。

```python
X = df.copy()
y = X.pop("price")

# Label encoding for categoricals
for colname in X.select_dtypes("object"):
    X[colname], _ = X[colname].factorize()

# All discrete features should now have integer dtypes (double-check this before using MI!)
discrete_features = X.dtypes == int
```

	Scikit-learn 的 feature_selection 模块中有两个互信息度量：一个用于实值目标（mutual_info_regression），另一个用于分类目标（mutual_info_classif）。我们的目标tatget_column价格是实值目标。下一个单元格将计算特征的 MI 分数，并将其封装在一个漂亮的数据框架中。

```python
from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]  # show a few features with their MI scores
```

![Pasted image 20240228223737](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040853745.png?imageSlim)

	And now a bar plot to make comparisions easier:

```python
def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")

plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)
```

	数据可视化是实用工具排名的重要后续手段。让我们仔细看看其中的几个。
	
	正如我们所预料的那样，高得分的路边权重特征与目标价格关系密切。

![Pasted image 20240228222752](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040853746.png?imageSlim)

	燃料类型特征的 MI 分数相当低，但我们可以从图中看到，它明显区分了马力特征中两个趋势不同的价格群体。这表明燃料类型产生了交互效应，可能并非不重要。在根据 MI 分数判定某个特征不重要之前，最好先研究一下任何可能的交互效应--领域知识在这方面可以提供很多指导。
	
![Pasted image 20240228223458](https://peiyihan-1324725457.cos.ap-beijing.myqcloud.com/Obsidian/202403040853747.png?imageSlim)

Data visualization is a great addition to your feature-engineering toolbox. Along with utility metrics like mutual information, visualizations like these can help you discover important relationships in your data. Check out our [Data Visualization](https://www.kaggle.com/learn/data-visualization) course to learn more!


#### MI and multicollinearity

This is about feature selection based on Mutual Information (MI) and multicollinearity:

• Even though we have a high MI for particular features, let's say 0.97 for one and 0.96 for another, we can't simply conclude that they are redundant based on the presence of multicollinearity. In such cases, we require domain expertise to make the correct decision.

• For instance, if the two features in question are "Celsius" and "Fahrenheit," domain expertise combined with the MI scores can guide us to remove one of these features.

What are your thoughts on this approach? Are there any other methods you think we should consider?其他考虑多重共线性的方法

##### Common techniques to detect multicollinearity

- **Correlation Analysis**: Calculate the _correlation matrix_ between the predictors to identify highly correlated variables. [Example](https://datagy.io/python-correlation-matrix/)
    
- **Variance Inflation Factor (VIF)**: Calculate the VIF for each predictor to quantify the degree of multicollinearity. A VIF greater than 5 or 10 (depending on the context) is often considered an indication of multicollinearity. [Example](https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/)
    

##### Techniques to Handle Multicollinearity

- **Feature Selection**: If multicollinearity is present, consider selecting a subset of the most relevant predictors based on domain knowledge or using feature selection techniques.
    
- **Combine Variables**: If two or more predictors are highly correlated, consider creating a composite variable that represents the shared information.
    
- **Principal Component Analysis (PCA)**: PCA can be used as a dimensionality reduction technique to create new uncorrelated variables (principal components) that explain most of the variation in the original predictors.
    
- **Regularization**: Regularization techniques like Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization) can handle multicollinearity to some extent. These methods add a penalty term to the loss function, which helps in stabilizing coefficient estimates and driving some of them closer to zero. Ridge Regression, in particular, is known to be effective against multicollinearity.