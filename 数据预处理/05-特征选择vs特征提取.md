### 特征选择方法综述
[scikit-learn中的特征选择方法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/141506312)
[如何为机器学习选择特征选择方法 - MachineLearningMastery.com](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)

### 1. 方差过滤

>适用对象：需要遍历特征或升维的算法
>主要目的：维持算法表现的前提下，帮助算法降低计算成本
>选取超参数threshold：每个数据集不一样，一定要选最优的超参数，可以画学习曲线，找模型最好的点，但现实往往不会这么做，因为非常耗时，只会使用阈值为0或者很小的方差来过滤，来率先消除一些明显用不到的特征，然后选择更优的特征选择方法继续削减特征数量

``` python
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold()
# 剔除零方差的属性
X_var0 = selector.fit_transform(X)

# 剔除方差小于中位数的属性
x_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
x_fsvar.shape
```
最近邻算法KNN、决策树、SVM、NN、回归算法，都需要便利特征或升维来进行运算，时间复杂度高，因此方差过滤的特征选择对他们尤其重要
但对于不需要遍历特征的算法，如rf，随机选取特征进行分支，本身运算就非常迅速，因此特征选择对他来说效果一般
——因为即使过滤法降低了特征的数量，随机森林也只会选取固定数量的特征来建模；而KNN对于更少的特征，模型明显会随着特征的减少而变得轻量

![[Pasted image 20240130113448.png]]

方差过滤并不一定会使效果提升（看运气），主要是为了提升效率


